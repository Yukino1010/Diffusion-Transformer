{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm \n",
    "from ema_pytorch import EMA\n",
    "from modules import *\n",
    "from diffusion import GaussianDiffusion\n",
    "\n",
    "from diffusers import AutoencoderKL\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from timm.models.vision_transformer import PatchEmbed, Attention, Mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiTBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, mlp_ratio=4.0, **block_kwargs):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "        self.attn = Attention(hidden_size, num_heads=num_heads, qkv_bias=True, **block_kwargs)\n",
    "        \n",
    "        mlp_hidden_dim = int(hidden_size * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=hidden_size, hidden_features=mlp_hidden_dim, \n",
    "                       act_layer=lambda: nn.GELU(approximate=\"tanh\"), drop=0)\n",
    "        \n",
    "        self.adaLN_modulation = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size, 6 * hidden_size, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # Calculate shift, scale, and gate values\n",
    "        mod_params = self.adaLN_modulation(c).chunk(6, dim=1)\n",
    "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = mod_params\n",
    "\n",
    "        # Attention block\n",
    "        modulated_norm_x = modulate(self.norm1(x), shift_msa, scale_msa)\n",
    "        x = x + gate_msa.unsqueeze(1) * self.attn(modulated_norm_x)\n",
    "\n",
    "        # MLP block\n",
    "        modulated_norm_x = modulate(self.norm2(x), shift_mlp, scale_mlp)\n",
    "        x = x + gate_mlp.unsqueeze(1) * self.mlp(modulated_norm_x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=32,\n",
    "        patch_size=2,\n",
    "        in_channels=4,\n",
    "        hidden_size=768,\n",
    "        depth=12,\n",
    "        num_heads=8,\n",
    "        mlp_ratio=4.0,\n",
    "        class_dropout_prob=0.1,\n",
    "        num_classes=25,\n",
    "        learn_sigma=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.learn_sigma = learn_sigma\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = in_channels * 2 if learn_sigma else in_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Embedding layers\n",
    "        self.x_emb = PatchEmbed(input_size, patch_size, in_channels, hidden_size, bias=True)\n",
    "        self.y_emb = LabelEmbedder(num_classes, hidden_size, class_dropout_prob)\n",
    "        self.t_emb = nn.Sequential(\n",
    "            SinusoidalPosEmb(256),\n",
    "            nn.Linear(256, hidden_size, bias=True),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size, hidden_size, bias=True),\n",
    "        )\n",
    "\n",
    "        # Positional embedding\n",
    "        num_patches = self.x_emb.num_patches\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, hidden_size), requires_grad=False)\n",
    "\n",
    "        # Transformer blocks and final layer\n",
    "        self.blocks = nn.ModuleList([\n",
    "            DiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio) for _ in range(depth)\n",
    "        ])\n",
    "        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels)\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        pos_embed = sincos_pos_embed(self.pos_embed.shape[-1], int(self.x_emb.num_patches ** 0.5))\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # Initialize adaLN modulation and final layer weights to zero\n",
    "        for block in self.blocks:\n",
    "            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)\n",
    "            nn.init.constant_(block.adaLN_modulation[-1].bias, 0)\n",
    "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].weight, 0)\n",
    "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].bias, 0)\n",
    "        nn.init.constant_(self.final_layer.linear.weight, 0)\n",
    "        nn.init.constant_(self.final_layer.linear.bias, 0)\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        # Reconstruct the image from patches\n",
    "        c = self.out_channels\n",
    "        p = self.x_emb.patch_size[0]\n",
    "        h = w = int(x.shape[1] ** 0.5)\n",
    "        x = x.reshape(x.shape[0], h, w, p, p, c)\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        return x.reshape(x.shape[0], c, h * p, h * p)\n",
    "\n",
    "    def forward(self, x, t, y):\n",
    "        x = self.x_emb(x) + self.pos_embed\n",
    "        c = self.t_emb(t) + self.y_emb(y, self.training)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, c)\n",
    "        x = self.final_layer(x, c)\n",
    "        return self.unpatchify(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            dataloader,\n",
    "            ckpt_dir,\n",
    "            load_path=None,\n",
    "            total_step=400000,\n",
    "            save_n_step=10000,\n",
    "            lr=1e-4,\n",
    "            timestep=1000\n",
    "        ):\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "        os.makedirs('./results/', exist_ok=True)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = nn.DataParallel(model).to(self.device)\n",
    "        \n",
    "        self.timestep = timestep\n",
    "        self.dataloader = dataloader\n",
    "        self.ckpt_dir = ckpt_dir\n",
    "        \n",
    "        self.step = 1\n",
    "        self.n_classes = model.num_classes\n",
    "        self.total_step = total_step\n",
    "        self.save_n_step = save_n_step\n",
    "        self.ema = EMA(self.model, beta = 0.995, update_every = 1)\n",
    "        self.ema.to(self.device)\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        #self.loss_fn = nn.L1Loss() \n",
    "        self.loss_history = []  # List to store loss values\n",
    "        self.scaler = torch.amp.GradScaler(self.device)\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.diff_method = GaussianDiffusion(self.device, timestep=timestep)\n",
    "\n",
    "        if load_path is not None:\n",
    "            self.load_state_dict(load_path)\n",
    "            self.ema.copy_params_from_ema_to_model()\n",
    "            print(\"sucessful load state dict !!!!!!\")\n",
    "            print(f\"start from step {self.step}\")\n",
    "    \n",
    "    def state_dict(self, step):\n",
    "        return {\n",
    "            \"step\": step,\n",
    "            \"ema\": self.ema.state_dict(),\n",
    "        }\n",
    "    \n",
    "    def load_state_dict(self, path):\n",
    "        state_dict = torch.load(path)\n",
    "        self.ema.load_state_dict(state_dict['ema'])\n",
    "        self.step = state_dict['step']\n",
    "\n",
    "    def train(self):\n",
    "        start = time.time()\n",
    "        print(f'Start of step {self.step}')\n",
    "        \n",
    "        for step in tqdm(range(self.step, self.total_step+1), desc=f\"Training progress\"):\n",
    "            self.optimizer.zero_grad()\n",
    "            img, label = next(iter(self.dataloader))\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # encoder the image into latent\n",
    "                # use AutoencoderKL.from_pretrained() class to construct pre-train model\n",
    "                latent = vae.encode(img).latent_dist.sample().mul_(0.18215)\n",
    "    \n",
    "            noise = torch.randn_like(latent)\n",
    "            t = torch.randint(0, self.timestep, (latent.shape[0],), device=self.device)\n",
    "            noisy_latent = self.diff_method.q_sample(latent, t, noise)\n",
    "            pred_noise = self.model(noisy_latent, t, label)\n",
    "\n",
    "            # compute the MSE between true noise (eps) and pred noise\n",
    "            loss = self.loss_fn(pred_noise, noise)\n",
    "            loss = loss.mean()\n",
    "\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.unscale_(self.optimizer)\n",
    "            self.grad_norm = nn.utils.clip_grad_norm_(self.model.parameters(), 1e9)\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            self.ema.update()\n",
    "            \n",
    "            self.loss_history.append(loss.item())\n",
    "            if step % self.save_n_step == 0:\n",
    "                #clear_output(wait=True)\n",
    "                epoch = step // self.save_n_step\n",
    "                time_minutes = (time.time() - start) / 60\n",
    "                torch.save(self.state_dict(step), f\"{self.ckpt_dir}/weight_epoch{epoch}.pt\")\n",
    "                \n",
    "                print(f\"epoch: {epoch}, loss: {loss.data} ~~~~~~\")\n",
    "                print (f'Time taken for epoch {epoch} is {time_minutes:.3f} min\\n') \n",
    "                print(f\"sucessful saving epoch {epoch} state dict !!!!!!!\")\n",
    "                start = time.time()\n",
    "\n",
    "                self.generate(epoch)\n",
    "                self.ema.copy_params_from_ema_to_model()\n",
    "        print(\"finish training: ~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    \n",
    "    def plot_loss(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.loss_history, label='Loss')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss over Steps')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'./training_loss.jpg')\n",
    "        plt.show()\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def generate(self, epoch):\n",
    "        _, label = next(iter(self.dataloader))\n",
    "        x_0 = self.diff_method.ddim_sample(self.ema,label[:9], self.n_classes)           \n",
    "        img = vae.decode(x_0 / 0.18215).sample\n",
    "\n",
    "        fake_img = img\n",
    "        num_rows = 3\n",
    "        num_columns = 3\n",
    "        \n",
    "        _, axs = plt.subplots(num_rows, num_columns, figsize=(6, 6))\n",
    "        for i in range(num_rows):\n",
    "            for j in range(num_columns):\n",
    "                ax = axs[i, j]\n",
    "                index = i * num_columns + j\n",
    "                img = fake_img[index]\n",
    "                img = unnormalize(img, device=self.device)\n",
    "                img = img.clamp(0, 1)\n",
    "\n",
    "                # Display the image\n",
    "                ax.imshow(img.permute(1, 2, 0).cpu().detach().numpy())\n",
    "                ax.axis('off')\n",
    "        plt.savefig(f'./{epoch}_result_img.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_dir = \"/kaggle/input/imagenet100/\" \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  \n",
    "    transforms.ToTensor(),         \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=os.path.join(imagenet_dir, \"train.X1\"), transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "TOTAL_ITERATION = 200000\n",
    "SAVE_N_ITERATION = 5000\n",
    "CKPT_DIR = './model_weight/'\n",
    "LOAD_PATH = None\n",
    "\n",
    "model = DiT(\n",
    "    input_size=32,\n",
    "    patch_size=2,\n",
    "    in_channels=4,\n",
    "    hidden_size=768,\n",
    "    depth=12,\n",
    "    num_heads=8,\n",
    "    mlp_ratio=4.0,\n",
    "    class_dropout_prob=0.1,\n",
    "    num_classes=25,\n",
    "    learn_sigma=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    train_loader,\n",
    "    CKPT_DIR,\n",
    "    load_path=LOAD_PATH, \n",
    "    total_step=TOTAL_ITERATION,\n",
    "    save_n_step=SAVE_N_ITERATION,\n",
    "    lr=LR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
